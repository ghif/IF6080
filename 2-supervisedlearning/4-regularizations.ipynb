{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all relevant modules\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import time as timer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(x, y, batch_size=32):\n",
    "    n  = x.shape[0] # number of data\n",
    "    batches = [] \n",
    "    batch_num = math.floor(n / batch_size)\n",
    "    print(f\"batch num: {batch_num}\")\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        x_batch = x[i * batch_size:(i+1) * batch_size, :]\n",
    "        y_batch = y[i * batch_size:(i+1) * batch_size]\n",
    "        batches.append((x_batch, y_batch))\n",
    "\n",
    "    if n % batch_size != 0:\n",
    "        x_batch = x[batch_num * batch_size:, :]\n",
    "        y_batch = y[batch_num * batch_size:]\n",
    "        batches.append((x_batch, y_batch))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticClassifier:\n",
    "    def __init__(self, step_size=1e-7):\n",
    "        self.step_size = step_size\n",
    "        self.w = None\n",
    "        self.eps = 1e-20\n",
    "    \n",
    "    def initialize_weights(self, d=1):\n",
    "        self.w = np.random.normal(0, 0.01, (d+1, ))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp((-z + self.eps)) + self.eps)\n",
    "\n",
    "    def model_fn(self, X):\n",
    "        return self.sigmoid(X @ self.w)\n",
    "    \n",
    "    def loss_fn(self, y, prob_hat, mode='mean'):\n",
    "        if mode == 'sum':\n",
    "            loss = -np.sum((y * np.log(prob_hat + self.eps) + (1 - y) * np.log(1 - prob_hat + self.eps)))\n",
    "        else:\n",
    "            loss = -np.mean((y * np.log(prob_hat + self.eps) + (1 - y) * np.log(1 - prob_hat + self.eps)))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def grad_loss_fn(self, X, prob_hat, y):\n",
    "        n = X.shape[0]\n",
    "        g = ((prob_hat - y) @ X) / n\n",
    "        return g\n",
    "\n",
    "    def accuracy_fn(self, y, prob_hat):\n",
    "        y_hat = np.round(prob_hat)\n",
    "        acc = np.abs(y - y_hat) < self.eps\n",
    "        return np.sum(acc) / acc.shape[0]\n",
    "\n",
    "    def train_on_step(self, X, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (nd.array): n x d\n",
    "            y\n",
    "        \"\"\"\n",
    "        Xb = np.insert(X, 0, 1, axis=1) # add 1s for bias term\n",
    "        yb = y\n",
    "\n",
    "        # Forward pass\n",
    "        probb_hat = self.model_fn(Xb, self.w)\n",
    "        \n",
    "        loss_val = self.loss_fn(yb, probb_hat)\n",
    "        \n",
    "        # Backward pass: gradient update\n",
    "        g = self.grad_loss_fn(Xb, yb, probb_hat)\n",
    "        self.w = self.w - self.step_size * g\n",
    "\n",
    "        return loss_val, probb_hat\n",
    "    \n",
    "\n",
    "    def fit(self, X, y, epochs=1, batch_size=32):\n",
    "        \n",
    "        batches = create_batch(X, y, batch_size=batch_size)\n",
    "        num_batches = len(batches)\n",
    "\n",
    "        d = X.shape[1]\n",
    "        self.initialize_weights(d=d)\n",
    "        \n",
    "        losses = []\n",
    "        for ep in range(epochs):\n",
    "            loss_avg = 0.\n",
    "            start_t = timer.time()\n",
    "            for b, (X_batch, y_batch) in enumerate(batches):\n",
    "                # Train on step or batch\n",
    "                l, prob_hat = self.train_on_step(X_batch, y_batch)\n",
    "\n",
    "                # Compute accuracy\n",
    "                acc = self.accuracy_fn(y_batch, prob_hat)\n",
    "\n",
    "                loss_avg += l\n",
    "\n",
    "            loss_avg /= num_batches\n",
    "            losses.append(loss_avg)\n",
    "            \n",
    "\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
